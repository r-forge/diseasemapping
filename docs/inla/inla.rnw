\documentclass[xcolor=dvipsnames]{beamer}

\usetheme{Singapore}
\usecolortheme[named=RawSienna]{structure}

\usefonttheme{serif}
\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{footline}[frame number] 

\usepackage[nogin]{c:/brownpe/share/Sweave}
\SweaveOpts{echo=FALSE,fig=true,prefix.string=Figures/G,height=2,width=3}

\graphicspath{{Figures/}}
\setkeys{Gin}{width=\textwidth}

\usepackage{amsmath}
\usepackage{c:/brownpe/share/Sweave}

\def\mm#1{\ensuremath{\boldsymbol{#1}}} % version: amsmath
\newcommand{\comment}[1]{}

\title{Generalized Linear Mixed Models}
\author{Patrick Brown}
\institute{Cancer Care Ontario, and \\ School of Public Health, University of Toronto}

\begin{document}
<<junk,fig=false>>=
options(SweaveHooks=list(fig=function() par(mar=c(2.5,2.5,0.1,0.1), mgp=c(1.5, 0.5, 0), cex=0.6)))
@

\frame{\maketitle}


\begin{frame}
\frametitle{Outline}
\begin{itemize}
	\item The model and the problem
	\item First generation of approximate methods and PQL
	\item Bayesian inference and Markov Chain Monte Carlo
	\item Integrated Nested Laplace Approximations --- the holy grail?
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{The model}
$Y_{ij}$ is the disease status of person $i$ in family $j$.
\begin{align*}
Y_{ij} \sim  & \text{Bernoulli}(p_{ij})\\\
logit(p_{ij}) = &x_{ij}\beta + V_{i}\\
V_{i} \sim &\text{N}(0, \sigma^2)
\end{align*}
$V_i$ is a family-level random effect.

\begin{itemize}
	\item What are the parameter estimates $\hat\sigma^2$ and $\hat\beta$?
	\item and their standard errors?
	\item And what are the predictors of family-level random effects $E(V_i | data)$, and their conditional distributions $[V_i | data]$.
\end{itemize}

\end{frame}

\begin{frame}[fragile]
\frametitle{Example: }
\begin{columns}
\column{0.5\textwidth}
\begin{itemize}
	\item Repeated measurements on the presence of a bacteria in children
	\item Placebo and two different treatments
\end{itemize}

<<thedata1,fig=false>>=
library(MASS)
data(bacteria)
bacterianew <- bacteria[,-(2:3)]
bacterianew$y = as.integer(bacteria$y=="y")
levels(bacterianew$trt) <- c("placebo", "drug", "drugplus")
bacteria=bacterianew

head(bacteria)
#$
@

\column{0.5\textwidth}
\begin{itemize}
\item the ID column denotes repeated measurements on each child.
	\item $Y_{ij}$ is the bacteria presence in child $i$ at time $j$
	\item $X_{ij}$ is a vector of indicator variables for treatment
	\item $V_i$ is each child's relative risk
\end{itemize}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Example: lung cancer in Ontario}
\begin{columns}
\column{0.65\textwidth}
\begin{itemize}
	\item $Y_i$ and $E_i$ are the observed and expected lung cancer cases in region $i$
	\item $X_i$ is income, a covariate
	\item $U_i$ and $V_i$ are spatial and independent random effects
\end{itemize}

\begin{align*}
Y_i \sim & \text{Posson}(\lambda_i E_i)\\
\log(\lambda_i) = & \mu + X_i \beta + U_i + V_i\\
U_i \sim &MRF(\sigma^2_u)\\
V_i \sim & N(0, \sigma^2_v)
\end{align*}

\column{0.35\textwidth}

Expected 
\includegraphics{ontarioE.jpg}
\comment{
library(diseasemapping)
library(sp)
data(popdata)
data(casedata)
model = getRates(casedata, popdata, ~age*sex)
ontario = getSMR(popdata,model,  casedata)
jpeg('Figures/ontarioE.jpg', height=600, width=800)
print(spplot(ontario, 'logExpected', lty=0))
dev.off()
}


Observed
\includegraphics{ontarioO.jpg}

\comment{
	jpeg('Figures/ontarioO.jpg', height=600, width=800)
 print(spplot(ontario, 'observed',lty=0))
dev.off()
}


\end{columns}

\end{frame}



\begin{frame}
\frametitle{Likelihood function?}
\begin{itemize}
	\item The $V_i$ are not parameters, and not observed, so they must be integrated out of the likelihood:
\end{itemize}
\[
L(\beta, \sigma) = \prod_i \int pr(Y_{ij}|V_i) pr(V_i) dV_i 
\]
\begin{itemize}
	\item The integral is intractable except in a few special cases
	\item such as Normally-distributed $Y_{ij}$ 
	\item or Gamma $Y_{ij}$ and $V_{i}$ with no covariates
	\item Empirical Bayes methods are based on conjugate priors
\end{itemize}
\end{frame}



\begin{frame}
\frametitle{First generation of approximations}

\begin{itemize}
	\item With no random effects, model fitting is done with Iterated Reweighted Least Squares, or Fisher Scoring
	\item For Poisson data, pretend $Y_i \sim N(\lambda_i, \lambda_i)$ and use least squares.  Though you need to know the variance to do this.
	\item Choose a starting value, then iterate between calculating the variances and estimating the parameters.
\end{itemize}


\begin{block}{Penalized Quazi-likelihood}
Start with initial values for $V_i$ and $\sigma$, then use normal-based likelihood and Fisher scores to:
\begin{itemize}
	\item estimate $\hat\beta$
	\item update predictions of  $V_i$, 
	\item Re-estimate $\sigma$
	\item repeat
\end{itemize}

\end{block}

\end{frame}

\begin{frame}[fragile]
\frametitle{Example: Bacteria}
{\scriptsize
<<bacteriaPQL,fig=false,echo=true>>=
library(MASS)
pql = glmmPQL(y ~ trt, random = ~1|ID, family='binomial',data=bacteria)
summary(pql)
@
}

\end{frame}

\begin{frame}
\frametitle{Advantages/Disadvantages}

\begin{block}{Advantages}
\begin{itemize}
	\item Quick and easy to use
	\item It's better than ignoring dependence completely and using a glm.
\end{itemize}
\end{block}

\begin{block}{Disadvantages}
\begin{itemize}
	\item It doesn't always produce good results
	\item Especially if your data are very non-normal,  with small numbers in each group.
	\item Theoretical justification isn't strong
\end{itemize}
\end{block}


\end{frame}


\begin{frame}
\frametitle{Markov Chain Monte Carlo}
\begin{itemize}
	\item Put prior distributions on parameters $[\sigma]$, $[\beta]$.
	\item Simulate from the posterior distributions
	\[
	[\sigma|Y],\ [\beta|Y],\ [V|Y]
	\]
	\item Use $[V|Y] = [Y|V][V]/[Y]$, but $[Y]$ is impossible to evaluate.
	\item Gibbs sampler: alternate between sampling from $[V|Y,\beta,\sigma]$ and $[\beta,\sigma|V,Y]$.
	\item Random walk Metropolis: update $V_i^{(p)}$ to  $V_i^{(p+1)}~N(V_i^{(p)}, \nu^2)$, accept the move with probability
	\[
	\min[1, pr(Y|V_i^{(p+1)}) /  pr(Y|V_i^{(p)})) ]
	\]
	\item This is guaranteed to converge to a sample from $[V_i|Y]$\ldots
	\item \ldots eventually
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Example:bacteria}
{\scriptsize
<<bacmcmc,fig=false,echo=true>>=
library(glmmBUGS)
bac = glmmBUGS(y ~ trt, effects='ID', family='bernoulli',data=bacteria, modelFile='modelx.bug')
startingValues = bac$startingValues
source("getInits.R")
library(BRugs)
library(R2WinBUGS)
bacResult = bugs(bac$ragged, getInits, 
	parameters.to.save = names(getInits()),
  n.chain=3, n.iter=100, n.burnin=10, n.thin=2,
  program="openbugs", model.file='modelBac.bug') 
bacParams = restoreParams(bacResult, bac$ragged)

@
}
\end{frame}

\begin{frame}[fragile]
\frametitle{Posterior samples}
<<bacSample,echo=true>>=
hist(bacParams$SDID)
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Parameter estimates}
<<bacpar,fig=false,echo=true>>=
bacSummary = summaryChain(bacParams)
bacSummary$scalars[,c(1:3,5,8)]
bacSummary$betas[,c(1:3,5,8)]
bacSummary$RID[1:4,c(1:3,5,8)]
#$
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Did the chiain converge?}
<<bacchain,echo=true>>=
checkChain(bacParams)
@

\end{frame}

\begin{frame}[fragile]
\frametitle{Example: Lung cancer in Ontario}
\begin{verbatim}
popDataAdjMat = poly2nb(ontario,row.names=as.character(ontario[["CSDUID"]]))

forBugs = glmmBUGS(formula=observed + logExpected ~ 1,
  effects="CSDUID",   family="poisson", 
  spatial=popDataAdjMat,
  data=ontario@data)
startingValues = forBugs$startingValues
source("getInits.R")

ontarioResult = bugs(forBugs$ragged, getInits, 
  parameters.to.save = names(getInits()),
    model.file="model.bug", n.chain=3, n.iter=50, 
    n.burnin=10, n.thin=2,
      program="winbugs", debug=T)

ontarioParams = restoreParams(ontarioResult, forBugs$ragged)
\end{verbatim}
\end{frame}

\comment{
	jpeg('Figures/ontarioCancer.jpg', height=600, width=800)

}

\begin{frame}[fragile]
\frametitle{Predicted cancer rates $E(U_i + V_i|Y)$}
\begin{verbatim}
ontarioSummary = summaryChain(ontarioParams)
ontario = mergeBugsData(popdata, ontarioSummary)
spplot(ontario, "RCSDUID.mean")
\end{verbatim}
\includegraphics[width=0.8\textwidth]{ontarioCancer.jpg}
\end{frame}

\begin{frame}
\frametitle{Integrated Nested Laplace Approximations}
\begin{itemize}
\item Notice that
\[
\[
\pi[\theta|Y] = \frac{\pi[U,\theta|Y]}{\pi[U|\theta,Y]}
\]
\item Laplace approximation
\[
\tilde\pi[\theta|Y] \propto \left. \frac{\pi[U,\theta|Y]}{\tilde\pi_G[U|\theta, Y]} \right| \theta=\theta^*
\]
where $\theta^*$ is the mode, and $\pi_G$ is a Gaussian approximation.
	\item \[
	\pi[U,\theta|Y] \propto \pi[\theta]\pi[U|\theta] \prod_{i} \pi[Y_i | U_i,\theta]
	\]

	
\end{itemize}
\end{frame}

\begin{frame}
\begin{itemize}
	\item Do the following integrals numerically
\end{itemize}
\frametitle{Marginal posteriors}
           \begin{align}
                \widetilde{\pi}(U_{i}\mid\mm{y}) &= \int
                \widetilde{\pi}(\mm{\theta} \mid \mm{y}) \;
                \widetilde{\pi}(U_{i} \mid \mm{\theta}, \mm{y})\; d\mm{\theta}\nonumber\\
                \widetilde{\pi}(\theta_{j} \mid \mm{y}) &= \int
                \widetilde{\pi}(\mm{\theta} \mid \mm{y}) \;
                d\mm{\theta}_{-j} \nonumber
            \end{align}
\end{frame}

\begin{frame}
    \frametitle{Remarks}
    \begin{enumerate}
    \item Expect $\widetilde{\pi}(\mm{\theta} | \mm{y})$ to be
        accurate, since
        \begin{itemize}
        \item $\mm{x}|\mm{\theta}$ is \emph{a priori} Gaussian
        \item Likelihood models are `well-behaved' so
            \begin{displaymath}
                \pi(\mm{x} \mid \mm{\theta}, \mm{y})
            \end{displaymath}
            is \emph{almost} Gaussian.
        \end{itemize}
    \item There are no distributional assumptions on $\mm{\theta} |
        \mm{y}$
    \item Similar remarks are valid to
        \begin{displaymath}
            \widetilde{\pi}(x_{i}\mid\mm{\theta}, \mm{y})
        \end{displaymath}
    \end{enumerate}
\end{frame}

\begin{frame}
\frametitle{How it works}
\begin{enumerate}
	\item Explore the space to find the model of $[\theta|Y]$
	\item Then compute the Laplace approximation for $[U|Y,\theta]$
	\item Then numerically integrate out the $\theta_i$.
	\item Use polynomials in place of the Gaussian to approximate the $[U_i|\theta,Y]$.
\end{enumerate}
\end{frame}

\begin{frame}[fragile]
\frametitle{Example: bacteria}
{\small
\begin{verbatim}
bacteria$r<-bacteria$y=="y"
bacteria$n<-1
formula1<- r ~ trt + f(ID,model="iid",param=c(0.001,0.001))
mod1 = inla(formula1,family="binomial",data=bacteria,Ntrials=n)
summary(mod1) #This will give summaries of the posteriors

Fixed effects:
                 mean        sd 0.025quant 0.975quant    kld dist
(Intercept)  2.2955447 0.4747077   1.498364  3.3689509 0.155881406
trtdrug     -1.2147419 0.5831969  -2.442864 -0.1223520 0.027425825
trtdrug+    -0.7292562 0.5917214  -1.953683  0.3977671 0.008737907

Model hyperparameters:
                mean   sd     0.025quant 0.975quant
Precision for ID 0.9724 0.5651 0.2943     2.4230  
Expected number of effective parameters(std dev) :18.03(7.335)
Number of equivalent replicates : 12.20
\end{verbatim}
}
\end{frame}

\begin{frame}
\frametitle{Posterior distribution of precision}
\includegraphics[width=0.7\textwidth]{Prec_BACTERIA.pdf}
\end{frame}

\begin{frame}[fragile]
\frametitle{Example: Lung cancer in Ontario}
{\small
\begin{verbatim}
nb2inla("ontario.graph",popDataAdjMat)
data$observed[is.na(data$logExpected)]<-NA
data$logExpected[is.na(data$logExpected)]<- -100 
data$region<-0:(length(data$CSDUID)-1)
data$region.spatial<-data$region

formula2<-observed ~ f(region.spatial,
    model="besag", graph.file="ontario.graph", 
    param=c(20,15),initial=2) + 
  f(region,model="iid", param=c(1,0.01))

mod2 <- inla(formula2,family="poisson",data=data,
  offset=logExpected,verbose=TRUE)

data$V<-mod2$summary.random$region[,2]
data$U<-mod2$summary.random$region.spatial[,2]

writePolyShape(ontario, "ontario")

\end{verbatim}
}
\end{frame}

\begin{frame}
\frametitle{Smoothed cancer risk}
\includegraphics{Map_LINEAR.pdf}
\end{frame}

\begin{frame}[fragile]
\frametitle{Results}
{\small

\begin{verbatim}
summary(mod2)

Fixed effects:
                mean       sd 0.025quant 0.975quant     kld dist
(Intercept) 0.7209629 3.654778  -6.456466   7.888327 7.134535e-05

Random effects:
Name      Model         Max KLD
region.spatial   Besags ICAR model   0.00083
region   IID model   0.01849

Model hyperparameters:
                            mean   sd     0.025quant 0.975quant
Precision for region.spatial 2.2255 0.3251 1.6469     2.9213  
Precision for region         1.9612 0.1960 1.6005     2.3700  
Expected number of effective parameters(std dev) :357.87(2.378)
Number of equivalent replicates : 1.115
\end{verbatim}

}
\end{frame}

\begin{frame}
\frametitle{Advantages/Disadvantanges}
\begin{block}{Advantages}
\begin{itemize}
	\item It appears that this is a very good approximation, and is much less computationally intensive than MCMC
	\item the code works in parallel, so is very fast on multi-core machines
\end{itemize}
\end{block}
\begin{block}{Disadvantages}
\begin{itemize}
\item Only marginals, not joint posteriors, are produced.
\item INLA is harder to modify than MCMC routines, though that might be only due to familiarity with MCMC
\item There are some things which INLA can't do, though in theory MCMC can do anything.
\item Starting values do have an effect, so INLA isn't fool-proof.
\end{itemize}
\end{block}
\end{frame}

\begin{frame}
\frametitle{Conclusions:}
\begin{itemize}
	\item There's no excuse for avoiding GLMM's
	\item or using PQL 
	\item Good models and good tools are available for dependent non-Gaussian data.
\end{itemize}
\end{frame}

\end{document}


