\documentclass[article,nojss]{jss}



<<downloadJssClassFile, eval=FALSE, include=FALSE>>=
zUrl = 'http://www.jstatsoft.org/public/journals/1/jss-style.zip'
zFile = basename(zUrl)
if(!file.exists(zFile))
        download.file(zUrl, zFile)
unzip(zFile, exdir='.',
                files = grep("^jss", unzip(zFile, list=TRUE)$Name, value=TRUE)
)
@


\usepackage{float}
%\usepackage{framed}
\usepackage{subcaption}
\renewcommand{\subfloat}[2][need a sub-caption]{ \subcaptionbox{#1}{#2} }
\usepackage{amsmath}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{listings}
\usepackage[shortcuts]{extdash}
\usepackage{booktabs}




%\addbibresource{paper2.bib}
\def\cov{{\text{COV}}}
\def\E{{\text{E}}}
\def\T{{\footnotesize{^{_{\sf T}}}}}
\setkeys{Gin}{width=0.45\textwidth}
\setcitestyle{square}

<<echo=FALSE,results="hide">>=
options(width=65)
options(scipen=999)
@


\newcommand{\class}[1]{`\code{#1}'}
\newcommand{\fct}[1]{\code{#1()}}


<<setup, cache=FALSE,include=FALSE>>=
library('knitr')
options(continue="+  ",  prompt="R> ", digits=3, width=60) 

opts_chunk$set(echo=TRUE,fig.path='Figures/G', fig.height=3,
                fig.width=4.5,dev='png',tidy=TRUE,fig.align='center',
                tidy.opts=list(blank=FALSE, width.cutoff=57),
                prompt=TRUE, highlight=FALSE,cache.stuff=1)

hook_output <- function(x, options) {
        if (knitr:::output_asis(x, options)) return(x)
        paste0('\\begin{CodeOutput}\n', x, '\\end{CodeOutput}\n')
}

knit_hooks$set(output = hook_output)
knit_hooks$set(message = hook_output)
knit_hooks$set(warning = hook_output)


knit_hooks$set(source  = function(x, options) {
        x = gsub("\n", "\n+", x)
        x = paste("R>",paste(x, collapse='\nR> '))
#       paste0(c('\\begin{example*}', x, '\\end{example*}', ''),
#                       collapse = '\n')
x =  paste0(c('\\begin{CodeInput}', x, '\\end{CodeInput}', ''),
                        collapse = '\n')
                        x               
})

knit_hooks$set(plot = function (x, options) 
                {
                        paste( hook_plot_tex(x, options), "\n", 
                                        sep = "")
                }
)

knit_hooks$set(chunk  = function(x, options) {
        if (knitr:::output_asis(x, options)) return(x)
        theend = gregexpr("end\\{Code(Input|Output)\\}", x)
        theend=theend[[1]]
        if(theend[1]>0){
                theend = theend[length(theend)] + attributes(theend)$match.length[length(theend)]
                x = paste(substr(x,1,theend-1),"\n\\end{CodeChunk}", substr(x,theend, nchar(x)))
                x=sub('begin\\{Code', 'begin{CodeChunk}\n\\\\begin{Code', x)
        }
        x
})
knitr::knit_hooks$set(margins = function(before, options, envir) {
    if (!before) 
        return()
    graphics::par(mar = c(1.5 + 0.9 * options$margins, 1.5 + 
        0.9 * options$margins, 0.2, 0.2), mgp = c(1.45, 0.45, 
        0), cex = 1.25)
})
opts_knit$set(header='')

@


<<preliminaries, echo=FALSE, results="hide", eval=FALSE>>=
  knitr::knit_hooks$set(plot=knitr::hook_plot_tex)  
  options(prompt = "R> ", continue = "+  ", width = 70, useFancyQuotes = FALSE)
knitr::opts_chunk$set(highlight=FALSE, background='#FFFFFF00', fig.height=4, fig.width=6, out.width='0.45\\textwidth')
library("MASS")
@

\author{Ruoyong Xu\\University of Toronto
   \And Patrick Brown\\University of Toronto
   \And Pierre L’Ecuyer\\University of Montr\'eal}
\Plainauthor{Ruoyong Xu, Patrick Brown, Pierre L’Ecuyer}


\title{A tool set for random number generation on GPUs in R}
\Plaintitle{}
\Shorttitle{}


\Abstract{
We introduce the \proglang{R} package \pkg{clrng} which leverages the \pkg{gpuR} package to generate random numbers on a Graphics Processing Unit (GPU) with the \pkg{clRNG} (OpenCL) library. Parallel processing with GPU's can speed up computationally intensive tasks, which when combined with \proglang{R}, it can largely improve \proglang{R}’s downsides in terms of slow speed, memory usage and computation mode.  %There is currently no \proglang{R} package that does random number generation on GPU, while random number generation is critical in simulation-based statistical inference and modeling. 
\pkg{clrng} enables reproducible research by setting random streams on GPU and can thus accelerate several types of simulation and modelling. This package is portable and flexible, developers can use its random number generation kernel for various other purposes and applications. 
}


\Keywords{GPU, \pkg{clrng} package, parallel computing, \pkg{clRNG} library}
\Plainkeywords{GPU, clrng package, parallel computing, clRNG library}


\Address{
  Ruoyong Xu, Patrick Brown\\
  Department of Statistics\\
  University of Toronto\\
  700 University Ave., Toronto, ON M5G 1Z5, Canada\\
  E-mail: \email{ruoyong.xu@mail.utoronto.ca}, \email{patrick.brown@utoronto.ca}\\\\ 
  Pierre L’Ecuyer\\
  Department of Computer Science and Operations Research\\
  University of Montr\'eal\\
  Pavillon André-Aisenstadt, 
   2920 chemin de la Tour, 
   Montréal, QC,  Canada,  H3T 1J4\\
  E-mail: \email{lecuyer@iro.umontreal.ca}}

\begin{document}
%\SweaveOpts{concordance=TRUE}





\section[Introduction]{Introduction}

% Why?
% 
% gpu massively parallel and relatively cheap, useful for stats
% there are gpu packages in R, gpuR
% no random numbers yet.  complicated to do on gpu, need multiple seeds becuase of parallel generation
% reproducability: need to save current state and restore

In recent years, parallel computing with \proglang{R} \citep{r2021} has become a very important topic and attracted lots of interest from researchers \citep[see][for a review]{eddelbuettel2021parallel}. Although \proglang{R} is one of the most popular statistical software with many advantages, it has drawbacks in memory usage and computation mode aspects \citep{zhao_2016}. To be more specific, (1) \proglang{R} requires all data to be loaded into the main memory (RAM) and thus can handle a very limited size of data; (2) \proglang{R} is a single-threaded program, it can not effectively use all the computing cores of multi-core processors. Parallel computing is the solution to these drawbacks, for an overview of current parallel computing approaches with \proglang{R}, see CRAN Task View by \citet{cran2021} at \url{https://cran.r-project.org/web/views/HighPerformanceComputing.html}. 

Graphics Processing Units (GPUs) have the potential to make an important contribution to parallel computing with \proglang{R}. GPUs can perform thousands of computations simultaneously, which makes them powerful for doing massively parallel computing, and they are relatively cheap compared to multicore CPU's. Although there have been a number of \proglang{R} packages developed which provide some GPU capability, they inevitably come with some limitations. Packages such as \pkg{gputools}, \pkg{gpumatrix}, \pkg{cudaBayesreg}, \pkg{rpud} (available on github), are no longer maintained, the popular \pkg{tensorflow} \citep{tensorflow1} package uses GPU via \proglang{Python}, which makes it difficult to include as a dependency for new \proglang{R} packages. All of these mentioned packages are restricted to \proglang{R} users with NVIDIA GPUs. \pkg{gpuR} \citep{gpur1} is the only \proglang{R} package with a convenient and flexible interface between \proglang{R} and GPUs, and it is compatible with many GPU devices. By utilizing the \pkg{ViennaCL} \citep*{rupp2016viennacl} library, it provides a bridge between \proglang{R} and non-proprietary GPUs through the \proglang{OpenCL} (Open Computing Language) backend, which when combined with \pkg{Rcpp} \citep{rcpp1} gives a building block for other \proglang{R} packages. 

Random number generation is critical in simulation-based statistical inference, machine learning and many other scientific fields. While most random number generators are sequential, the \proglang{R} packages \pkg{parallel}, \pkg{future} \citep*{future1.19.1} and \pkg{rlecuyer} \citep{sevcikova2015package} are able to generate random numbers in parallel on multicore CPUs. More specifically, \pkg{parallel} writes an interface for the \pkg{RngStreams}, a \proglang{\texttt{C++}} library by \cite{l2002object} which is based on a combined multiple-recursive generator (MRG) MRG32k3a. \pkg{future} and \pkg{rlecuyer} also uses the combined MRG algorithm for generating random numbers. For up-to-date review papers on the generation of random numbers on parallel devices, and GPUs in particular, see:  \cite{rLEC15a,rLEC17p,rLEC21a}.


The \pkg{clrng} package described here is currently the only \proglang{R} package that is able to generate random numbers on GPUs. Accomplishing this is complicated because each process must produce an independent (non-overlapping) sequence of random numbers, and in order to ensure reproducibility it should be possible to save and restore the current state of each stream of random numbers at any point.  Here we introduce the \proglang{R} package \pkg{clrng} that leverages the \pkg{gpuR} package, and can generate random numbers on GPU by using the \proglang{clRNG} \citep{l2015clrng} library in \proglang{OpenCL}, and can thus accelerate several types of statistical simulation and modelling.



The remaining sections are organized as follows:
In Section 2, we introduce streams and the use of streams in work-items on a GPU device for generating uniform random numbers, and the usage of \fct{clrng::runif}.
In Section 3, we introduce two non-uniform RNGs in \pkg{clrng}, as examples for users to develop other RNGs of interest on GPUs in \proglang{R}.
In Section 4, we apply GPU-generated uniform random numbers in Monte Carlo simulation for Fisher’s exact test, we introduce how the random numbers are used and how the algorithm is parallelized and implemented on GPU. Then we provide two real data examples to demonstrate the function usage and its \proglang{R} performance.
In Section 5, we show a useful application of normal random numbers on GPU, using them to simulate batches of Gaussian random surfaces with Mat\'ern covariance matrices simultaneously, the simulation also uses GPU-enabled functions from our other package \pkg{gpuBatchMatrix}.
Finally, the paper concludes with a short summary and a discussion in Section 6.




<<echo=FALSE, results="hide", message=FALSE>>=
library("gpuR")
setContext(   grep('gpu', listContexts()$device_type) [1]    )
#setwd("/home/ruoyong/diseasemapping/pkg/clrng/inst/documents/paper1_2021_5")
@

\section{Uniform random number generation} \label{}
Uniform random number generators (RNGs) are the foundation for simulating random numbers from all types of probability distributions. \cite{l2012random} summarized the usual two steps to generate a random variable in computational statistics: (1) generating independent and identically distributed (i.i.d.) uniform random variables on the interval $(0, 1)$, (2) applying transformations to these i.i.d. $U(0, 1)$ random variables to get samples from the desired distribution. \cite{l2012random} and \cite{robert2004random} present many general transformation methods for generating non-uniform random variables, for example, the most frequently used inverse transform method, the Box-Muller algorithm \citep{box1958note} for Gaussian random variable generation, and so on, which are all built on uniform random variables. %In the rest paper, ``RNG'' refers to uniform pseudo random number generator.


\proglang{clRNG} is an \proglang{OpenCL} library for uniform random number generation, it provides four different RNGs: the MRG31k3p, MRG32k3a, LFSR113, and Philox-4×32-10 generators. These four RNGs use different types of constructions. The \pkg{clrng} package uses the MRG31k3p RNG, making it able to generate random numbers on GPUs. We choose MRG31k3p as the base generator for the following reasons:  the original \pkg{RNGStreams} package \citep{l2002object} was built with MRG32k3a, which was designed to be implemented in double precision, and not with 32-bit integers. The MRG31k3p generator was designed later, specifically for 32-bit integer arithmetic, so it runs faster on the 32-bit GPUs. It is also faster than Philox-4×32-10. The MRG31k3p generator was introduced in the paper \citep{rLEC00b}. MRG31k3p was also statistically tested extensively and successfully, \cite[see][]{rLEC07b}.    




In what follows, we will illustrate how to create streams and how to use streams to generate uniform random numbers. 

%THE PROBLEM:  each process needs to be independent, there's a risk of overlap.  solution: streams.  mrg31kp3 a stream is a vector of three positive integers.  initial state is first stream, compute new streams sequentially.
\subsection{Creating streams}\label{createstreams}
When a RNG is called in parallel processes or successively called at several places in a program in \proglang{R}, random number generation would be more complicated because there is no guarantee that the streams (analogous to \code{.Random.seed} in \proglang{R}) do not overlap, and thus the generated sequences of random numbers is possible to have correlation. \pkg{clrng} uses multiple distinct streams that are used in work-items that executes in parallel on a GPU device. A popular way of obtaining multiple streams is to take an RNG with a long period and cut the RNG's sequence into very long disjoint pieces of equal length $Z$, and use each piece as a separate stream. Creating a new stream amounts to computing its starting point. Each of these streams can also be partitioned into substreams with equally-spaced starting points \citep{l2002object, rLEC15a}, although this is not currently implemented in \pkg{clrng}. In general, a stream object contains three elements: the current state of the stream, the initial state of the stream (or seed), and the initial state of the current substream (by default it is equal to the seed). Streams are created sequentially in the way that whenever the user creates a new stream, the software automatically jumps ahead by $Z$ steps to find its initial state, and the three states in the stream object are set to it. 


In the \proglang{clRNG} library, the MRG31k3p RNG's entire period of length approximately $2^{185}$ is divided into approximately $2^{51}$ non-overlapping streams of length $Z = 2^{134}$. %Each stream can be further partitioned into substreams of length $2^{72}$. 
The state (and seed) of each stream is a vector of six 31-bit integers. This size of state is appropriate for having streams running in work-items on GPU cards, while providing a sufficient period length for most applications. The initial state of the first stream (also called ``initial seed of the package'' or ``initial seed of the stream creator'' in \proglang{clRNG}) for the MRG31k3p RNG is by default $(12345, 12345, 12345, 12345, 12345, 12345)$. 


%The function \fct{clrngMrg31k3pCreateStreams} from \proglang{clRNG} creates streams on the host using the MRG31k3p RNG. If we want to use the streams in work-items on a GPU device, the streams have to be copied from the host to the global memory of the GPU device, and then each work-item copies the current states only of the streams to its private memory. 
\fct{setBaseCreator} sets the initial state of the first stream that is created, it plays a role in \pkg{clrng} like the function \fct{SetPackageSeed} in \pkg{RngStreams} and \fct{clrngSetBaseCreatorState} in \pkg{clRNG}. \pkg{clrng} is able to create streams both on the host and on the GPU device. The \fct{createStreamsCpu} function does the former. The \proglang{R} output below creates 4 streams on the host. 
%is an interface function of \fct{clrngMrg31k3pCreateStreams} that creates streams on host as \proglang{R} matrices, see the \proglang{R} output below, where we created 4 streams on the host using \fct{createStreamsCpu}, after transposing \code{myStreamsCpu}, each column of the matrix is a stream, creating streams on host makes it easy for us to view and check the streams behavior. Substreams are not created in \pkg{clrng} because currently we use one stream per work-item. 
<<createStreams_CPU, message=FALSE>>=
# creating streams on CPU
library("clrng")
setBaseCreator(rep(12345,6))
myStreamsCpu <- createStreamsCpu(4)
t(myStreamsCpu)
@
\fct{setBaseCreator} has one argument:
\begin{itemize}
\itemsep0em 
  \item \code{initial}: a vector of six 31-bit integers, recycled if length is shorter. Default is set to \code{c(12345, 12345, 12345, 12345, 12345, 12345)}.
\end{itemize}
\fct{createStreamsCpu} has one argument:
\begin{itemize}
\itemsep0em 
  \item \code{n}: number of streams to create.
\end{itemize}
We move streams to the GPU by converting them to a `\code{vclMatrix}'.
<<createStreams_Gpu, eval=FALSE>>=
myStreamsGpu = vclMatrix(myStreamsCpu)
@
Equivalently, \fct{createStreamsGpu} creates streams directly on the GPU and returns a `\code{vclMatrix}', which makes it slightly more efficient when the number of streams is large.
<<createStreams_Gpu2>>=
myStreamsGpu2 = createStreamsGpu(4)
@
%Users should run either \fct{createStreamsCpu} or \fct{createStreamsGpu} at the beginning of a task and retain these streams for all of the random number generation the task requires.  Creating new streams with different initial states introduces the very small possibility of overlap between the original and new streams. If an \proglang{R} session is restarted during a task, perhaps because a preliminary analysis is done following an initial set of simulations and the full set of simulations continued at a later date, the state of the streams should be transferred to the CPU and saved to disk.  The `\code{initial}' argument to the stream creator should be used with caution and we recommend using the default.  It bears repeating that two sets of streams with different initial states may produce overlapping numbers, the probability is small but increases when the number of streams is large.
There are a few important notes on the usage of these above functions. (1), We allow users to call \fct{setBaseCreator}  at any time. \fct{setBaseCreator} should be called once before \fct{createStreamsCpu} or \fct{createStreamsGpu} if they would like to select their own initial seeds, or never, in which case streams take the package's default initial seed. (2), Users may switching between \fct{createStreamsCpu} and \fct{createStreamsGpu} in the same program, however, this is not recommended as it causes unnecessary data transfer between host and device. (3), If an \proglang{R} session is restarted during a task, as long as users saved the \proglang{R} environment, the initial states (seeds) of the newly created streams will carry on from the last streams' seed from the previous \proglang{R} session.

%A newly created stream should not have the same seed as another existing stream, just like when you create a new car, it must have a distinct serial number. Streams should be created only once, and moved or copied if needed. And also it causes unnecessary data transfer between host and device if users switch often between \fct{createStreamsCpu} and \fct{createStreamsGpu}.


%However, any data is lost when the \proglang{R} session is terminated. %\fct{createStreamsGpu} is adapted from the clrngMrg31k3pCreateStreams() and creates streams directly on a GPU device. Setting a seed same as the MRG31k3p default seed in \fct{createStreamsGpu}, we produce on device exactly the same streams as those created on host as follows. Notice that the code is just for showing \fct{createStreamsGpu} and \fct{createStreamsCpu} are \proglang{R} interface functions for \fct{clrngMrg31k3pCreateStreams} on GPU and CPU respectively, in practice it is not recommended to create a new stream that has the same initial seed as another existing stream in one simulation program.


%also has the two arguments \code{n} and \code{initial}. \fct{createStreamsGpu} is similar to \code{.Random.seed} in \proglang{R}. By setting the RNG status with the \proglang{R} function \fct{set.seed}, we are able to repeat the sequence of random seeds, which is an important quality criteria of RNGs. Similarly in \fct{createStreamsGpu} and \fct{createStreamsCpu}, with the \code{initial} argument, we make the sequence of streams repeatable.






\subsection{Generating uniform random numbers}

The streams created sequentially are used by work-items (the GPU analogy of CPU cores) on a GPU to generate random numbers. In \pkg{clrng} each work-item takes one distinct stream to generate random numbers, so the number of streams should always be at least the number of total work-items in use. The main part of the kernel (functions for execution on the GPU) for generating uniform random numbers is shown in Listing \ref{lst:uniformkernel}. Kernels are written in the \proglang{C}-like \proglang{OpenCL} language, in which \code{__kernel} declares a function as a kernel, and the \code{__global} prefix to the pointer kernel arguments specifies that they point to global memory space accessible to all work items.  Users can set the argument \code{verbose=2} in the random number generator to print out the kernel, which is slightly more complex than the code in Listing \ref{lst:uniformkernel}. 
  
Here the pointers \code{streams} and \code{out} refer to the streams and the  output matrix respectively, which are stored in global memory. \code{Nrow} and \code{Ncol} represent row and column number of the matrix \code{out} respectively.   \code{Npad} is the "internal" number of columns, and \code{out} is an \code{Nrow} by \code{Ncol} submatrix of a larger \code{Nrow} by \code{Npad} matrix.

Each stream's current state is copied to the private memory of each work-item by the function \fct{streamsToPrivate}, in which \code{g1} and \code{g2} point to the first three and second three elements of stream states.
The object \code{startvalue} gives the position of the work item's stream in the \code{streams} object and is computed with the
\code{NpadStreams} object defined in a macro not shown here.   

The function \fct{clrngMrg31k3pNextState} generates an uniform random integer between 1 and 2147483647,  and then scaled to be in the interval $(0,1)$ by multiplying it by a constant \code{mrg31k3p\_NORM\_cl} (defined to be $1/2147483648$). If to generate random integers, \code{temp} is not scaled. At the end of generating random numbers, streams are transferred back to global memory through the function \fct{streamsFromPrivate}.

\begin{lstlisting}[language=C,basicstyle=\small,label={lst:uniformkernel}]
__kernel void mrg31k3pMatrix(
  __global int* streams,
  __global float* out,
  int Nrow, int Ncol, int Npad){

int Drow, Dcol;
uint g1[3], g2[3];
double temp, fact = mrg31k3p_NORM_cl;
const int startvalue = (get_global_id(0) + 
get_global_size(0) * get_global_id(1)) * NpadStreams;

streamsToPrivate(streams,g1,g2,startvalue);

for(Drow = get_global_id(0); Drow < Nrow;
    Drow += get_global_size(0)){

    for(Dcol = get_global_id(1); Dcol < Ncol; 
        Dcol += get_global_size(1)){

        temp = fact * clrngMrg31k3pNextState(g1, g2);
        out[Drow * Npad + Dcol] = temp;

  }//Dcol
}//Drow

streamsFromPrivate(streams,g1,g2,startvalue);

}//kernel
\end{lstlisting}

%Figure \ref{fig:1} illustrates how this kernel executes in practice with a simple example. Suppose we have a $2 \times 4$ NDRange ((a) in Figure \ref{fig:1}) and want to create a $4 \times 6$ matrix ((b) in Figure \ref{fig:1}) of uniform random numbers. In each grid of (a) that represents a work-item, there are two rows of numbers, the number in the above row is the stream index (from 0 to 7), and a pair of numbers in the second row is the 2-dimensional global ID of the work-item. Each stream object is passed to each work-item. The number in a cell of the matrix corresponds to the stream index, it indicates which stream generates a random number for this matrix cell. %For example, stream 0 which is allocated in work-item $(0,0)$ generates a random number in matrix $(0,0)$ in the first iteration, then moves on to matrix $(0,2)$ and generates a random number there in the second iteration, in the third iteration it generates a random number in matrix $(0,4)$ and so on until it goes out of the matrix range in the next iteration.
% \begin{figure}[ht]
%     \centering
%     \subfloat[]{{\includegraphics[width=8cm]{/home/ruoyong/diseasemapping/pkg/gpuRandom/inst/documents/paper1_2021_5/f1} }}
%     \qquad
%     \subfloat[]{{\includegraphics[width=8cm]{/home/ruoyong/diseasemapping/pkg/gpuRandom/inst/documents/paper1_2021_5/f2} }}
%     \caption{Using streams to produce a $4 \times 6$ matrix of random U(0,1) numbers. \label{fig:1}}
% \end{figure}

Now we use the 4 streams created in section \ref{createstreams} to generate a vector of double-precision i.i.d. $U(0,1)$ random numbers with \fct{clrng::runif}. To view the generated random numbers, we need to convert them to \proglang{R} vectors or matrices, by doing this, the random numbers are moved from the global memory of the GPU device to the host.
<<simStuff2, eval=TRUE,tidy=TRUE, echo=TRUE>>=
as.vector(clrng::runif(n=6, streams=myStreamsGpu2, Nglobal=c(2,2)))
@
The arguments of the \fct{clrng::runif} are described as follows:
\begin{itemize}
\itemsep0em 
  \item \code{n}: A vector of length 2 specifying the row and column number if to create a matrix, or a number specifying the length if to create a vector.
  \item \code{streams}: Streams for random number generation, streams cannot be missing.
  \item \code{Nglobal}: Global index space. Default is set as $(64,8)$.
  \item \code{type}: ``double'' or ``float'' or ``integer'' format of generated random numbers.
  \item \code{verbose}: Print extra information if $\text{verbose} > 1$. Default is FALSE.
\end{itemize}
%\fct{runif} returns the same sequence of random numbers because it uses the same streams \code{myStreamsGpu} and \code{myStreamsGpu2} here. 
Reusing \code{myStreamsGpu} will produce a vector different from \code{sim_1}. Because each time a random number is generated, the current state of the stream advances by one position. %Below shows the \code{myStreamsGpu} after it being used.
<<eval=FALSE,tidy=TRUE, include=FALSE>>=
t(matrix(as.matrix(myStreamsGpu), nrow(myStreamsCpu), ncol(myStreamsCpu), dimnames = dimnames(myStreamsCpu)))
@
Unlike the objects created by \proglang{R}, by default they remain in the memory after a restart. Streams created on GPU does not remain in the memory when a new \proglang{R} session begins. There are two ways to reproduce results, depending on how the program calls streams creator. The simpler way to reproduce is just to restart the program with the same initial seed for the creator.  But it is important that the program always creates the streams in exactly the same order, for example, like the toy \proglang{R} program for demonstration below, we can reproduce the random matrix \code{sim_mat} just by keeping a record of the initial seed \code{c(11,22,33,44,55,66)}.
<<showreproduce1, echo=TRUE, eval=FALSE>>=
library('clrng')
setBaseCreator(c(11,22,33,44,55,66))
sim_mat <- matrix(0, nrow=10, ncol= 6)
for (i in 1:10){
  if( i %% 2 == 0){
    streams1 <- createStreamsGpu(4)
    sim_mat[i,] <- as.vector(clrng::rnorm(n=6, streams=streams1, Nglobal=c(2,2)))
  }else if(i==3){
    streams2 <- createStreamsGpu(4)
    sim_mat[i,] <- as.vector(clrng::runif(n=6, streams=streams2, Nglobal=c(2,2)))
  }else{
    streams3 <- createStreamsGpu(8)
    sim_mat[i,] <- as.vector(clrng::rexp(n=6, streams=streams3, Nglobal=c(4,2)))
}
}
sim_mat
@
For more complicated applications, we recommend users to save the matrix of streams (current states and initial states) on the CPU in a file as a \code{.rds} object. And later recall the saved streams for regenerating results, the streams will start from their current states after they are loaded. This is a safer way than the previous one for reproducing results in simulations. Below shows the code that saves streams to a data file called \code{myStreams.rds} on CPU and then load it back and transfer it to a `vclMatrix' \code{streams_saved} on GPU, and using it to (re)generate some Normal random numbers.
<<save streams,echo=TRUE,eval=FALSE>>=
saveRDS(as.matrix(myStreamsGpu2), "myStreams.rds")
# Load the streams object as streams_saved
streams_saved <- vclMatrix(readRDS("myStreams.rds"))
clrng::rnorm(n=6, streams=streams_saved, Nglobal=c(2,2))
@

%\subsection{Plot the uniform random numbers (now deleted?)}







\section{Some non-uniform random number generation}
\subsection{Normal random number generation}
We apply the Box-Muller transformation to $U(0,1)$ random numbers to generate standard normal random numbers. As shown in Algorithm \ref{algorithm1}, Box-Muller algorithm takes two independent, standard uniform random variables $U_1$ and $U_2$ and produces two independent, standard Gaussian random variables $X$ and $Y$, where $R$ and $\Theta$ are polar coordinate random variables. %The derivation basically uses transformation from Cartesian coordinates to polar coordinates to represent two independent, Normally distributed random variables. 
The Box-Muller algorithm is a very good choice for Gaussian transform on GPU compared to other transform methods \citep{howes2007efficient}, because this algorithm has no branching or looping, which are the things GPU is not good at. %but GPU is good at the high computational load of sine and cosine functions in the algorithm. 

\begin{algorithm}[ht] 
\SetAlgoLined
 1, Generate $U_1$, $U_2$ i.i.d. from $U (0,1)$ \;
 2, Define \begin{align*} 
& R = \sqrt{-2*\log U_1},\\
&  \Theta = 2\pi*U_2,\\
&  X=R*\cos(\Theta),\\
& Y=R*\sin(\Theta);\;\end{align*}
 3, Take $X$ and $Y$ as two independent draws from $N(0,1)$\;
 \caption{Box-Muller algorithm.}
 \label{algorithm1}
\end{algorithm}


Listing \ref{lst:normal} shows a fragment of the kernel that generates standard Gaussian random numbers. Aside from using local memory and the part that does the Box-Muller transformation, this kernel is the same with the kernel for generating uniform random numbers shown in Listing \ref{lst:uniformkernel}. If developers want to use other Gaussian transform methods, this kernel is easy to be adapted and incorporated in other \proglang{R} packages. This kernel executes over a 2-dimensional index space with $1 \times 2$ work-groups. \code{part[0]} and \code{part[1]} correspond to $R$ and $\Theta$ in the formulas respectively. \code{PI_2} is defined to be $\pi/2$. As the work-items in a work-group proceed at different rates, \code{barrier(CLK_LOCAL_MEM_FENCE)} ensures correct ordering of memory operations to local memory, so that Gaussian random numbers $(X_1,Y_1), \dots, (X_n,Y_n)$ are generated in pairs correctly, errors such as $(X_n, Y_{(n-1)})$ or $(X_{(n-1)}, Y_{(n)})$ are avoided.
%

\begin{lstlisting}[language=C,basicstyle=\small,label={lst:normal},breaklines=true, escapeinside={(*@}{@*)}]
__kernel void mrg31k3pMatrix(
  __global int* streams,
  __global double* out,
  int Nrow, int Ncol, int Npad, int NpadStreams){

int Drow, Dcol;
uint g1[3], g2[3];
local double part[2];
int startvalue = (get_global_id(0) * get_global_size(1) + 
get_global_id(1)) * NpadStreams;

double sinOrCosPart1, addForSine = - get_local_id(1) * PI_2;
double temp, fact = mrg31k3p_NORM_cl;
if(get_local_id(1)){
  fact = TWOPI_mrg31k3p_NORM_cl;
}

streamsToPrivate(streams,g1,g2, startvalue);

for(Drow=get_global_id(0); Drow < Nrow;
    Drow += get_global_size(0)){

    for(Dcol=get_global_id(1); DcolBlock < Ncol; 
        Dcol += get_global_size(1)){
        
      temp = fact * clrngMrg31k3pNextState(g1, g2);
      part[get_local_id(1)] = temp;
      
      if(!get_local_id(1)) {
        part[0] = sqrt(-2.0*log(part[0]));
      }
      barrier(CLK_LOCAL_MEM_FENCE);
      
      // sine for local[1], cos for local[0]
      sinOrCosPart1 = cos(part[1] + addForSine);
      out[Drow * Npad + Dcol] = part[0]*sinOrCosPart1;

      barrier(CLK_LOCAL_MEM_FENCE);
  }//Dcol
}//Drow
streamsFromPrivate(streams,g1,g2,startvalue);
}//kernel
\end{lstlisting}

%We illustrate in Figure \ref{fig3} how this kernel executes in practice with the previous toy example: to create a $4 \times 6$ matrix of Gaussian random numbers using a $2 \times 4$ NDRange. %Each stream is copied to the private memory of each work-item. 
%Work-items are organized to be in 1 by 2 work-groups as the random numbers are generated in pairs using the Box-Muller method. The shading on the grids in the NDRange distinguishes work-groups. The numbers in the third row in each gird of NDRange represents the local index of each work-item. Each work-group generates a pair of Gaussian random numbers \code{(X,Y)} in each iteration, where work-item of `\code{get_local_id(1)}$=0$' generates the \code{X} of a pair, and work-item of `\code{get_local_id(1)}$=1$' generates the \code{Y} of a pair. %Here with the loop iterating over columns, the cells of matrix are filled from left to right, that is, in the first iteration, the 8 work-items generate random numbers in the leftmost two columns (8 cells) of the matrix simultaneously, the second iteration fills random numbers in the middle two columns of the matrix, and the third iteration fills the rest two columns of the matrix simultaneously. %Therefore, each work-item generates one Gaussian random number in each iteration, The matrix filling process is the same as that shown in Figure \ref{fig:1}. 

% \begin{figure}[ht]%
%     \centering
%     \subfloat[]{{\includegraphics[width=8cm]{/home/ruoyong/diseasemapping/pkg/gpuRandom/inst/documents/paper1_2021_5/f3} }}%
%     \qquad
%     \subfloat[]{{\includegraphics[width=8cm]{/home/ruoyong/diseasemapping/pkg/gpuRandom/inst/documents/paper1_2021_5/f4} }}%
%     \caption{Using streams to produce a $4 \times 6$ matrix of normal random numbers.}%
%     \label{fig3}
% \end{figure}
 

We generate a large-size matrix of 100 million double-precision Gaussian random numbers, and compare the run-time between using \fct{stats::rnorm} and \fct{clrng::rnorm}. The best performance we have seen using \fct{clrng::rnorm} is above 160 times faster with $512 \times 128$ work-items than using the \fct{stats::rnorm}. The difference in elapsed time becomes larger when matrix size goes larger.
<<random normal time compare, eval=TRUE, cache=FALSE>>=
streams <- createStreamsGpu(n = 512 * 128)
system.time(clrng::rnorm(c(10000,10000), streams=streams, Nglobal=c(512,128), type="double"))
@
<<random normal time compare2, eval=TRUE, cache=TRUE>>=
system.time(matrix(stats::rnorm(10000^2),10000,10000))
@


\subsection{Exponential random number generation}
The exponential random variates are produced by applying the inverse transform method on i.i.d. $U(0,1)$ random numbers. The random variable $X \sim \text{Exponential}(\lambda)$ has cumulative distribution function $F_{X}(x)=1-e^{-\lambda x}$ for $x \geq 0$ and $\lambda > 0$. The inverse of $F_{X}(\cdot)$ is $F^{-1}_{X}(y)= -(1/ \lambda) \log (1-y)$, for $ 0 \leq y <1$. The kernel is not shown because it is mostly same as the kernel for uniform and normal random numbers, except for the part that does the inverse transform. %Developers can make use of our kernel for generating other types of random variables on GPU in \proglang{R}.

Below is an example that produces a $2 \times 4$ matrix of Exponential random numbers with expectation equal to 1.
<<random exponential, eval=TRUE, cache=FALSE>>=
r_matrix <- clrng::rexp(c(2,4), rate=1, myStreamsGpu2, Nglobal=c(2,2), type="double")
as.matrix(r_matrix)
@
The arguments of the \fct{clrng::rexp} are described as follows:
\begin{itemize}
\itemsep0em 
  \item \code{n}: A vector of length 2 specifying the row and column number if to create a matrix, or a number specifying the length if to create a vector.
  \item \code{streams}: Streams for random number generation, streams cannot be missing.
  \item \code{Nglobal}: Global index space. Default is $(64,8)$.
  \item \code{type}: ``double'' or ``float'' format of generated random numbers.
  \item \code{verbose}: Print extra information if $\text{verbose} > 1$. Default is FALSE.
\end{itemize}





\section{An application of uniform RNG: Fisher's simulation}
The GPU-generated random numbers can be applied in suitable statistical simulations to accelerate the performance.
One application of GPU-generated random numbers in \pkg{clrng} is Monte Carlo simulation for Fisher's exact test. Fisher’s exact test is applied for analyzing usually $2 \times 2$ contingency tables when one of the expected values in table is less than 5. Different from methods which rely on approximation, Fisher's exact test computes directly the probability of obtaining each possible combination of the data for the same row and column totals (marginal totals) as the observed table, and get the exact p-value by adding together all the probabilities of tables as extreme or more extreme than the one observed. However, when the observed table gets large in terms of sample size and table dimensions, the number of combinations of cell frequencies with the same marginal totals gets very large, \cite[][p. 23]{mehta2011ibm} shows a $5 \times 6$ observed table that has 1.6 billion possible tables. Calculating the exact P-values may lead to very long run-time and can sometimes exceed the memory limits of your computer. Hence, the option \code{simulate.p.value = TRUE} in \fct{stats::fisher.test} is provided, which enables computing p-values by Monte Carlo simulation for tables larger than 2 $\times$ 2. 

The test statistic calculated for each random table is $-\sum_{i,j}\log(n_{ij}!), i=(1,\dots, I), j=(1,\dots,J)$, (i.e., minus log-factorial of table), where $I$ and $J$ are the row and column number of the observed table. This test statistic can also be independently calculated for a table by \fct{clrng::logfactSum}. Given an observed table and a number of replicates $B$, the Monte Carlo simulation for Fisher's exact test does the following steps: 
\begin{enumerate}
\setlength\itemsep{0em}
  \item Calculate the test statistic for the observed table.
    \item In each iteration, simulate a random table with the same dimensions and marginal totals as the observed table using the \fct{rcont2} algorithm, compute and sometimes save the test statistic from the random table. 
    \item Count the number of iterations (\emph{Counts}) that have test statistics less or equal to the one from the observed table.
    \item Estimate p-value using $\frac{1 + \emph{Counts}}{B + 1}$.
\end{enumerate}

Step 1 and step 2-3 are done on a GPU with two kernels enqueued sequentially. For step 2, \fct{clrng::fisher.sim} adapted the function \fct{rcont2} used by \fct{stats::fisher.test} for constructing random two-way tables with given marginal totals. The algorithm \citep[see][]{patefield1981efficient} samples the entries row by row, one at a time, conditional on the values of the entries already sampled. The conditional probabilities for the possible values of the next entry are updated dynamically each time a new entry is sampled. Then this entry is sampled by standard inversion of the cumulative distribution function, using one $U(0,1)$ random number.  For an $I \times J$ table, this requires $(I-1)(J-1)$ random numbers (the last column and last row do not need to be sampled). Finally, one can compute the test statistic for this newly sampled table. On a GPU, this step can be replicated say $n$ times in parallel by creating $n$ distinct random streams and launching $n$ separate work-items. Each work-item takes a random stream as input, performs all of Step 2 and 3, and returns the value of the test statistic on the GPU. Computing the p-value on the CPU (Step 4) is then a trivial operation.  Step 2 of saving test statistics from the random tables is made optional, which can reduce the run-time. By the way, there are a lot of other more recent methods for sampling (larger) contingency tables, many of them use Markov Chain Monte Carlo, the use of streams and GPU would be quite different in that case. See for examples \citep{10.1214/13-AOS1131, KAYIBI2018298, dyer1997sampling}. Doing the \fct{fisher.sim} function on GPU opens up many possibilities for future work, the specific implementation we've done for the tables isn't necessarily the optimal one. 

The arguments of the \fct{clrng::fisher.sim} are described as follows:
\begin{itemize}
\setlength{\parskip}{0pt}
  \item \code{x}: A contingency table, a ``vclMatrix'' object.
  \item \code{N}: Number of simulation runs.
  \item \code{streams}: Streams for random number generation, streams cannot be missing.
  \item \code{type}: ``double'' or ``float'' of returned test statistics.
  \item \code{returnStatistics}: If TRUE, return all test statistics. 
  \item \code{Nglobal}: Global index space. Default is set as $(64,16)$.
\end{itemize}
Users request $N$ number of replicates, while the actual number of replicates to be executed on GPU is larger (\code{ceiling(N/prod(Nglobal))*prod(Nglobal)}). We show the advantage of \fct{clrng::fisher.sim} by computing the p-values for two real data examples: one with a relatively big p-value and another with a very small p-value, and compare the run-time with using \fct{stats::fisher.test} for each of the data sets on two computers: one with a very good CPU and an ordinary GPU, the other is equipped with an excellent GPU and an ordinary CPU. The \proglang{R} outputs for testing on computer 2 is shown in the following. 


 
\subsection{Comparing run-time: Month data example}\label{fisher_month}
The 2-way contingency tables consist of selected data from the 2018 Natality public use data \citep{National2018} from the Centers for Disease Control and Prevention’s National Center for Health Statistics (NCHS), the 2018 natality data file may be downloaded at \url{https://www.cdc.gov/nchs/data_access/VitalStatsOnline.htm}. Table \ref{tab:month} is a $12 \times 12$ table that shows frequencies for congenital anomalies of the newborn by birth month in 2018 within the United States. The column variables of these two tables represent the twelve categories of congenital anomalies of the newborn: 1) Anencephaly; 2) Meningomyelocele/Spina bifida; 3) Cyanotic congenital heart disease; 4) Congenital diaphragmatic hernia; 5) Omphalocele; 6) Gastrochisis; 7) Limb reduction defect; 8) Cleft lip with or without cleft palate; 9) Cleft palate alone; 10) Down syndrome; 11) Suspected chromosomal disorder; and 12) Hypospadias. The first 5 rows of the table are displayed below.
 <<Timecomparemonth,echo=FALSE,eval=TRUE, cache=FALSE, results="hide">>=
 # datamonth<-read.csv("/home/ruoyong/diseasemapping/pkg/gpuRandom/inst/documents/paper1_2021_5/month.csv")
 # month<-as.matrix(datamonth[,-1])
 # rownames(month) <- c("Jan", "Feb","Mar","Apr","May","Jun","Jul","Aug","Sept","Oct","Nov","Dec")
 # colnames(month) <- c("Ane", "Men", "Cya", "Her", "Omp", "Gas", "Lim", "Cle", "Pal", "Dow", "Chr", "Hyp")
 month <- as.matrix(readRDS(system.file("extdata", "month.Rds", package = "clrng")))
 @
 
 <<monthdata,eval=TRUE,echo=FALSE, cache=TRUE, fig.pos='h', message=FALSE,purl=FALSE>>=
library(knitr)
library(tidyverse)
library(kableExtra)
knitr::kable(month,format="latex", align = c("rrrrrrrrrrrr"), #label="tab:month"
  caption = "Monthly birth anomaly data\\label{tab:month}", booktabs=TRUE,linesep = "") %>%
  kable_styling(full_width = F, position = "center")#latex_options = "HOLD_position")
@
<<monthDataForScript, include=FALSE, purl=TRUE>>-
knitr::kable(month)
@

 <<timecomparemonth1Gpu, eval=TRUE,cache=FALSE>>=
 streams <- createStreamsGpu(n =256*64)
 month_gpu<-vclMatrix(month,type="integer")
 system.time(result_month <- clrng::fisher.sim(month_gpu, 1e6, streams=streams,
                    type="double", returnStatistics=TRUE,  Nglobal = c(256,64)))
 result_month$threshold
 result_month$simNum
 result_month$counts
 result_month$p.value
 @
We got 410825 cases whose test statistics are below the observed threshold based on an actual number of 1015808 simulations, so the p-value from \fct{clrng::fisher.sim} for this table is about 0.40443, which is close enough to the p-value from \fct{stats::fisher.test}. \fct{clrng::fisher.sim} takes about 0.31 seconds, \pkg{clrng} accounts for 2.17\% of the elapsed time on CPU. 
 <<timecomparemonth3Gpu, eval=TRUE,echo=FALSE, cache=TRUE>>=
 month_stats <- as.vector(result_month$sim)
 @
 
 <<timecomparemonth4Cpu, cache=TRUE, eval=TRUE>>=
 #using CPU
 system.time(result_monthcpu<-stats::fisher.test(month,simulate.p.value = TRUE, B=1015808))
 result_monthcpu$p.value
 @



\subsection{Comparing run-time: Week data example}\label{fisher_week}
Table \ref{tab:week} is a $7 \times 12$ table shows frequencies for congenital anomalies of the newborn by birth day of week in 2018 within the United States. 
<<Timecompareweek, echo=FALSE, eval=TRUE, cache=TRUE,results="hide", fig.pos='h'>>=
# dataweek<-read.csv("/home/ruoyong/diseasemapping/pkg/gpuRandom/inst/documents/paper1_2021_5/weekday.csv")
# week<-as.matrix(dataweek[,-1])
# rownames(week) <- c("Mon", "Tue","Wed","Thu","Fri","Sat","Sun")
# colnames(week) <- c("Ane", "Men", "Cya", "Her", "Omp", "Gas", "Lim", "Cle", "Pal", "Dow", "Chr", "Hyp")
week <- as.matrix(readRDS(system.file("extdata", "week.Rds", package = "clrng")))
@

<<weekdata, eval=TRUE, echo=FALSE, cache=TRUE>>=
knitr::kable(week,format="latex",align = c("rrrrrrrrrrrr"),  #label = "tab:week",
             caption = "Day-of-week birth anomaly data\\label{tab:week}", booktabs=TRUE,linesep = "") %>%
kable_styling(full_width = F, position = "center")#, latex_options = "HOLD_position")
@


<<TimecompareweekGpu2, eval=TRUE, cache=FALSE>>=
week_GPU<-gpuR::vclMatrix(week,type="integer")
system.time(result_week<-clrng::fisher.sim(week_GPU, 1e7, streams=streams,
                type="double",returnStatistics=TRUE,Nglobal = c(256,64)))
result_week$threshold
result_week$simNum
result_week$counts
result_week$p.value
@
The ``week'' table has a much smaller p-value: around 0.000125, which should require a larger number of simulations to get a more accurate p-value. With more than ten million simulations, we get 1205 cases and a p-value around 0.000120472 by \fct{fisher.sim}. \fct{stats::fisher.test} takes 92.45 seconds, while \fct{fisher.sim} takes about 1.95 seconds, the GPU elapsed time is decreased to about 2.10\% of the time taken on CPU. 
<<TimecompareweekGpu3, echo=FALSE, eval=TRUE, cache=TRUE>>=
result_week$cpu = as.vector(result_week$sim)
@

<<TimecompareweekCpu, cache=TRUE, eval=TRUE>>=
#using CPU
system.time(result_weekcpu<-fisher.test(week,simulate.p.value = TRUE,B=10010624))
result_weekcpu$p.value
@


\subsection{A summary of the results}
We summarized the comparison results in Table \ref{tab:summary} and plot the test statistics in Figure \ref{fig4}.
<<summarycompare, echo=FALSE, eval=TRUE, cache=TRUE,fig.pos='H'>>=
library(kableExtra)
dt <- data.frame(
  var1 = c("B", "1M", "10M", "1M", "10M"),
  var2 = c('Intel 2.5ghz', 0.403804, 0.0001251, 10.74,  63.24),
  var3 = c('AMD Radeon', 0.403507, 0.0001274,   2.28,    10.82),
  var4 = c('Intel 3.7ghz', 0.4035606,0.0001202, 14.80,  92.00),
  var5 = c('NVIDIA V100', 0.403507,0.0001274,   0.32,   1.95),
  var6 = c('Data','month','week','month','week')
)

knitr::kable(dt, col.names = NULL, caption = "Summary of comparions of Fisher's test simulation on different devices. Computer 1 is equipped with CPU Intel Xenon W-2145 3.7Ghz and AMD Radeon VII. Computer 2 is equipped with VCPU Intel Xenon Skylake 2.5Ghz and VGPU Nvidia Tesla V100.\\label{tab:summary}") %>%
  kable_styling(full_width = FALSE, position = "center", latex_options = "HOLD_position") %>%
  kableExtra::group_rows(index = c("P-value" = 2, "Run-time" = 2)) %>%
  add_header_above(c(" " = 1, "Computer 1" = 2, "Computer 2" = 2, " " = 1))
@



<<fighistMonth, eval=TRUE, echo=FALSE, dev='pdf', fig.cap = "\\label{fig4}Approximate sampling distributions of the test statistics from the two examples Month and Week. The test statistic values of the observed tables are marked with a blue vertical line on each plot.", fig.subcap = c('Month data', 'week data'), out.width="0.47\\textwidth", margins=1,fig.pos='H'>>=
hist(month_stats, xlab="test statistic",#TeX('-$\\sum(\\log(n_{ij}!))$'),
     breaks=40, ylab="proportion", main="", prob=TRUE)
abline(v = result_month$threshold, col = "blue", lwd = 1.5)

hist(result_week$cpu, xlab='test statistic', ylab="proportion", prob=TRUE, main="")
abline(v = result_week$threshold, col = "blue", lwd = 1.5)
@







\section{An application of normal RNG: Gaussian surface simulation} 
Given a parameter space $X$, a random field $U$ on $X$ is a collection of random variables $\{U_x :x\in X\}$. A Gaussian random field is a random field with the property that for any positive integer $n$ and any set of locations $x_1,\dots,x_n \in X$, the joint distribution of $U=(U_{x_1},\dots,U_{x_n})^\top$ is multivariate Gaussian. The expectation and covariance function of $U$ completely determine the distribution of $U$, 
\begin{gather*} 
U = [U(x_1), \ldots, U(x_n)]^\top \sim \text{MVN}(0, \Sigma), \\
\Sigma_{ij} = \cov[ U(x_i),U(x_j) ] = \sigma^2 \frac{2^{\kappa-1}}{\Gamma(\kappa)} \left(\sqrt{8\kappa} \frac{|x_i = x_j|}{\phi}\right)^\kappa  K_\kappa\left(\sqrt{8\kappa}  \frac{|x_i = x_j|}{\phi}\right),
\end{gather*}
where \text{MVN} standards for the multivariate normal distribution, and $\Sigma$ standards for the covariance matrix of $U$. A common choice for $\Sigma$ is the Mat\'ern covariance \citep{matern1960spatial}. The Mat\'ern covariance between $U(x_i)$ and $U(x_j)$ takes the above form,
%$d =\|x-x'\|$ is the Euclidean distance between two spatial points, 
where $\sigma^2$ the variance of the random field $U$.
$\Gamma(\cdot)$ is the standard gamma function.  
$\phi$ is the range parameter or scale parameter with the dimension of distance, it controls the rate of decay of the correlation as distance increases. $K_\kappa(\cdot)$ is the modified Bessel function of the second kind with order $\kappa$, $\kappa$ is the shape parameter which determines the smoothness of $U(x)$. %specifically, $U(x)$ is $m$ times mean-square differentiable if and only if $\kappa > m$. %(For $\kappa=0.5$, the Mat\'ern covariance function reduces to the exponential covariance function $\sigma^2*\exp(-d/\phi)$, when $\kappa \rightarrow \infty$, $\Sigma(d) \rightarrow \sigma^2*\exp{-(\|d\|/\phi)^2}$, which is called the Gaussian covariance.)  
There are several alternative parameterisations of Mat\'ern covariance functions in literature  \citep[see][]{haskard2007anisotropic}. In \pkg{clrng}, we use the above form for computing Mat\'ern covariance.


%Due to the importance of Gaussian random field, there are a large amount of studies devoted to Gaussian random field generation techniques. 
\cite{LiuandLi2019} gives a comprehensive review on seven popular methods for Gaussian random field generation, %which are the turning bands method \citep{matheron_1973}, spectral method \citep{Meja1974OnTS,1972JSV}, matrix decomposition method \citep{davis1987}, Karhunen-Lo\'eve expansion \citep{LoeveM:1978}, moving average method \citep{journel1974, oliver1995moving}, sequential simulation \citep{johnson1987multivariate, gomez1993joint, pebesma2004multivariable}, and local average subdivision \citep{fenton1990simulation}. 
all of these methods except the matrix decomposition method, are approximations to the random field and have specific requirements on the type of grid or covariance functions. The matrix decomposition method is exact, it works for all covariance functions and can generate random fields on arbitrary grids, and is straightforward to implement \citep{LiuandLi2019}. Other \proglang{R} packages that offer simulation of Gaussian random fields such as \pkg{geoR} \citep{geoR2001}, does not work for large number of locations. The \pkg{RandomFields} \citep{RandomFields2015} package has several different methods for simulation of Gaussian fields, among which the circulant embedding, which is an improved method on direct matrix decomposition, and some variants of the method like the cut-off embedding \citep{gneiting2006fast} are also exact and fast for isotropic Gaussian random fields, however, they work only on rectangular grids. \pkg{clrng} is able to simulate on the GPU exact Gaussian fields on arbitrary grids as it relies on the matrix decomposition method. The implementation of this method is as follows.

%it can generate random fields on arbitrary position Circulant embedding is a fast simulation method for stationary (possibly anisotropic) Gaussian ran- dom fields on regular grids based on Fourier transformations. Cut-off embedding is a fast simulation method for stationary, isotropic Gaussian random fields on square lattices based on the standard RPcirculant method, so that exact simulation is garantueed for further covariance models

Suppose $U=(U_1, U_2, \dots, U_n)^\top$ is a Gaussian random field with mean $\mu$ and covariance matrix $\Sigma$, without loss of generality, we assume mean value zero $(\mu = 0)$. Since covariance matrix $\Sigma$ is symmetric and positive-definite, we can take Cholesky decomposition of $\Sigma$, then we can simulate random samples from $U$ using Algorithm \ref{algorithm2},

\begin{algorithm}[H]
\SetAlgoLined
 1, Calculate the covariance matrix $\Sigma$ between locations\;
 2, Compute the Cholesky decomposition of $\Sigma = L \cdot D \cdot L^\top$, where $L$ is a lower unit triangular matrix, and $D$ is a diagonal matrix\;
 3, Generate on GPU a random matrix $Z=(Z_1, Z_2, \dots, Z_n) \sim \text{MVN}(0,I_n)$, where $I_n$ is a $n \times n$ identity matrix\;
 4, Compute the random samples from $U$ in batches using $U = L \cdot D^{\frac{1}{2}} \cdot Z$ \;
 \caption{Gaussian random fields simulation using covariance matrix decomposition method.}
 \label{algorithm2}
\end{algorithm}

% $U=L*D^{1/2}*Z$, where 
% \begin{itemize}
% \itemsep0em 
% \item $L$ is the lower unit triangular matrix in Cholesky decomposition of $\Sigma = L*D*L^\top$,
% \item $D$ is a diagonal matrix.
% \item $Z=(Z_1, Z_2, \dots, Z_n) \sim \text{MVN}(0,I_n)$, where $I_n$ is a $n \times n$ identity matrix.
% \end{itemize}
The downside of this method is that it's computationally expensive and is thus very slow. When the number of locations $n$ is large, the time cost of Cholesky decomposition of the covariance matrix is $\mathcal{O}(n^3)$,  and is $\mathcal{O}(n^2)$ for the matrix-vector multiplication $L*Z$ \citep{LiuandLi2019}.  Our other \proglang{R} package \pkg{gpuBatchMatrix} is able to address this issue since it computes batches of Mat\'ern covariance matrices in parallel, and does Cholesky decomposition and matrix-matrix multiplication in batches in parallel on GPU. The following shows an example, in which we simulate on GPU eight Gaussian random fields of Mat\'ern covariance at one time with four sets of parameters, by taking advantage of the GPU-capabilities offered by \pkg{clrng} and \pkg{gpuBatchMatrix} together.
%\pkg{clrng} package not only does the matrices decomposition (Cholesky decomposition and matrix-vector multiplication) in parallel, but also computes the covariance matrices in parallel on GPU. 

\subsection{Simulating Gaussian random fields with Mat\'ern covariances}
<<setupData, eval=TRUE,echo=TRUE, message=FALSE>>=
library("gpuBatchMatrix")
library('geostatsp')
@
Step 1, set up a $51 \times 80$ raster using the `swissBorder' coordinates from \pkg{geostatsp}.
<<setupcoords, eval=TRUE>>=
NlocalCache = 1000
Nglobal = c(128, 64, 2)
Nlocal = c(4, 2, 2)
theType = "double"
@

<<swissRainBoundary, echo=TRUE>>=
data("swissRain", package="geostatsp")
myRaster = geostatsp::squareRaster(swissBorder, 80)
myRaster
@
Step 2, create four parameter sets as a small example. 
<<setup paramsBatch, eval=TRUE,echo=FALSE>>=
params = 
rbind(c(shape=1.25, range=50*1000, variance = 1.5, nugget = 0,anisoRatio = 1, anisoAngleRadians = 0), 
c(shape=2.15, range=60*1000, variance = 2, nugget = 0, anisoRatio = 4, anisoAngleRadians = pi/7),
c(shape=0.6, range=30*1000, variance = 2, nugget = 0, anisoRatio = 2, anisoAngleRadians = pi/7),
c(shape=3, range=30*1000, variance = 2, nugget = 0, anisoRatio = 2, anisoAngleRadians = pi/7)
)
@
<<show parameterBatch,eval=TRUE>>=
params
@
Step 3, compute the Mat\'ern covariance matrices using \fct{gpuBatchMatrix::maternBatch}, the returned Mat\'ern covariance matrices %$\Sigma_1, \Sigma_2, \dots, \Sigma_5$ correspond to the 5 parameter sets respectively, the matrices 
are each of size $4080 \times 4080$ and are stacked by row in the output \code{maternCov}. 
<<r outputBatchCreate, eval=TRUE>>=
maternCov = gpuBatchMatrix::maternBatch(
  params, myRaster,          
  Nglobal=c(128,64), Nlocal=c(16,4))
dim(maternCov)
@
Step 4, perform Cholesky decomposition on \code{maternCov}. The first argument in \fct{cholBatch} specifies the object to take Cholesky decomposition. Computed unit lower triangular matrices $L_i$'s are stacked by row in \code{maternCov}. The diagonal values of each $D_i$ are returned and stored in each row of \code{diagMat}. So if each batch $\Sigma_i$ is of size $n \times n$, then each batch $D_i$ is $1 \times n$.
<<r cholBach, eval=TRUE>>=
diagMat = gpuBatchMatrix::cholBatch(maternCov, Nglobal= c(128, 8), Nlocal= c(32, 8))
@
\begin{gather}
\begin{bmatrix} \Sigma_{1} \\ \Sigma_2 \\ \Sigma_3 \\ \vdots
\end{bmatrix}
 \rightarrow
 \begin{bmatrix}
  L_{1} \\ L_{2} \\L_{3} \\ \vdots
  \end{bmatrix} \text{and}
  \begin{bmatrix}
  D_{1} \\ D_{2} \\D_{3} \\ \vdots
  \end{bmatrix}
\end{gather}
Step 5, create some streams on GPU to generate two standard Gaussian random vectors \code{zmatGpu}$=(Z_1, Z_2)$ using \fct{clrng::rnorm}, in which \code{c(nrow(materCov),2)} specifies the number of rows and columns of \code{zmatGpu}.
<<r randomNormalsSim1, eval=TRUE, cache=FALSE>>=
streamsGpu <- createStreamsGpu(128*64)
@
<<r randomNormalsSim2, eval=TRUE, cache=FALSE>>=
zmatGpu = clrng::rnorm(
                c(nrow(maternCov),2), streams=streamsGpu, 
                Nglobal=c(128,64),
                type = theType)
@
Step 6, compute $U = L * D^{(1/2)}* Z$ in batches, see the following illustration for the matrix operation that \fct{multiplyLowerDiagonalBatch} does.  \code{maternCov}, \code{diagMat}, and \code{zmatGpu} correspond to the matrices $L$, $D$ and $Z=(Z_1, Z_2)$ respectively.
<<r multLowerdiag_LDZ, eval=TRUE, cache=FALSE, results='hide'>>=
simMat = gpuBatchMatrix::multiplyLowerDiagonalBatch(maternCov, 
  diagMat, zmatGpu,
  diagIsOne = TRUE,   
  transformD = "sqrt", 
  Nglobal=c(128, 64, 2), 
  Nlocal= c(8, 2, 1), 
  NlocalCache=1000)
@
\begin{gather}
 \begin{bmatrix}  L_{1} \\ L_{2} \\L_{3} \\ \vdots
 \end{bmatrix} 
 *
  \begin{bmatrix}
   D_{1} \\ D_{2} \\D_{3} \\ \vdots
   \end{bmatrix} 
   *
   \begin{bmatrix}
   Z_{11} & Z_{12}
   \end{bmatrix}
  =
 \begin{bmatrix}
   L_1D_1Z_{11} & L_1D_1Z_{12} \\
   L_2D_2Z_{11} & L_2D_2Z_{12} \\
   L_3D_3Z_{11} & L_3D_3Z_{12} \\
   \vdots  &   \vdots
  \end{bmatrix}
\end{gather}
Step 7, Finally, plot the ten realizations.
<<r plotSimSetup, echo=TRUE, eval=TRUE, cache=FALSE>>=
simRaster = raster::brick(myRaster, nl = ncol(simMat)*nrow(params))
values(simRaster) = as.vector(as.matrix(simMat))
@

<<r theSubcapName, include=FALSE>>=
names(simRaster) = apply(expand.grid('par',1:nrow(params), 
                                     'sim', 1:ncol(simMat)), 1, paste, collapse='')
theSubcap = gsub("par", "parameter ", names(simRaster))
theSubcap = gsub("sim", ", simuation ", theSubcap)
@

<<maternplot, echo=FALSE, eval=TRUE, cache=FALSE, dev='png', out.width = '0.47\\textwidth', fig.height=3, fig.width=4.5, fig.cap="Simulated Gaussian random fields", fig.subcap = theSubcap, fig.align='default'>>=
myCol = mapmisc::colourScale(breaks=sort(unique(c(-6, -4, seq(-2, 2), 4, 6))), style='fixed', col='Spectral')
for(D in names(simRaster)) {
  mapmisc::map.new(simRaster)
  plot(simRaster[[D]], legend=FALSE, add=TRUE, col=myCol$col, breaks=myCol$breaks)
  plot(swissBorder, add=TRUE)
}
mapmisc::legendBreaks("right", myCol, inset=0)
@






\section{Discussion}
The package \pkg{clrng} has been created to make GPU-generated uniform and some non-uniform random numbers accessible for \proglang{R} users, it enables reproducible research in simulations by setting initial seeds in streams and saving and reloading the current states of streams. We further applied the GPU-generated random numbers in suitable statistical simulations, such as the Monte Carlo simulation for Fisher’s exact test, and exact Gaussian spatial surfaces simulation, for which we are able to calculate quantiles for the normal distribution, our package \pkg{gpuBatchMatrix} adds additional help in the second application by taking the computational work for batches of Mat\'ern covariance matrices, Cholesky decomposition and matrix multiplication on GPU. %Most of these functions uses local memory on GPU. 
Comparison of performance between using \pkg{clrng} and using classic \proglang{R} on CPU for some real data examples has demonstrated significant improvement in execution time. 

By leveraging the \pkg{gpuR} package, \pkg{clrng} provides a user-friendly interface that bridges \proglang{R} and \proglang{OpenCL}, users can use the facilities in our package without the need to know the complex \proglang{OpenCL} or even the \proglang{\texttt{C++}} code.  \pkg{clrng} is portable as its backend \proglang{OpenCL} supports multiple types of processors, and it is also flexible as its kernels can be incorporated or reconstructed in other \proglang{R} packages for other applications. 

%One of the main difficulties in developing the package is bug fixes and uncertain software behaviors. Error messages are often vague and it is time-consuming to locate the error within the backend \proglang{C} program. The usual steps that we take to debug is commenting out some parts of codes and then recompile many times until we find the problem, or printing out flag messages in several places in the code to find out where the program stops at due to error. 

\pkg{clrng} is limited by the number and type of \proglang{OpenCL} RNGs used and the features of the \pkg{gpuR} package, as it depends upon the \pkg{gpuR} package. If developers wants to use our package to do something that's not supported in \pkg{gpuR}, for example ``sparse'' class objects, they would have to write \proglang{OpenCL} code to implement it.

Future work on \pkg{clrng} package could be: (1) Explore other RNGs, for example the MRG32k3a, LFSR113, and Philox-4×32-10 generators in \proglang{clRNG} library, and compare the \proglang{R} performance between using different GPU RNGs. (2) Now that the package can do Cholesky decomposition and matrix multiplication in batches on GPU, which is a motivation for us to work on parallel likelihood evaluations on GPU for Gaussian spatial models in the next step. (3) Create a ``sparse'' matrix class on GPU and use it for simulating Gaussian random fields with sparse correlation structures such as the Gaussian Markov random fields.





\bibliography{paper1}




\newpage












\begin{appendix}

\end{appendix}


\end{document}




